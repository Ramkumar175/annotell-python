(window.webpackJsonp=window.webpackJsonp||[]).push([[10],{107:function(e,t,n){"use strict";n.r(t),n.d(t,"MDXContext",(function(){return c})),n.d(t,"MDXProvider",(function(){return h})),n.d(t,"mdx",(function(){return f})),n.d(t,"useMDXComponents",(function(){return l})),n.d(t,"withMDXComponents",(function(){return p}));var a=n(0),i=n.n(a);function r(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function o(){return(o=Object.assign||function(e){for(var t=1;t<arguments.length;t++){var n=arguments[t];for(var a in n)Object.prototype.hasOwnProperty.call(n,a)&&(e[a]=n[a])}return e}).apply(this,arguments)}function s(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function m(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?s(Object(n),!0).forEach((function(t){r(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):s(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function d(e,t){if(null==e)return{};var n,a,i=function(e,t){if(null==e)return{};var n,a,i={},r=Object.keys(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||(i[n]=e[n]);return i}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(i[n]=e[n])}return i}var c=i.a.createContext({}),p=function(e){return function(t){var n=l(t.components);return i.a.createElement(e,o({},t,{components:n}))}},l=function(e){var t=i.a.useContext(c),n=t;return e&&(n="function"==typeof e?e(t):m(m({},t),e)),n},h=function(e){var t=l(e.components);return i.a.createElement(c.Provider,{value:t},e.children)},u={inlineCode:"code",wrapper:function(e){var t=e.children;return i.a.createElement(i.a.Fragment,{},t)}},b=i.a.forwardRef((function(e,t){var n=e.components,a=e.mdxType,r=e.originalType,o=e.parentName,s=d(e,["components","mdxType","originalType","parentName"]),c=l(n),p=a,h=c["".concat(o,".").concat(p)]||c[p]||u[p]||r;return n?i.a.createElement(h,m(m({ref:t},s),{},{components:n})):i.a.createElement(h,m({ref:t},s))}));function f(e,t){var n=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var r=n.length,o=new Array(r);o[0]=b;var s={};for(var m in t)hasOwnProperty.call(t,m)&&(s[m]=t[m]);s.originalType=e,s.mdxType="string"==typeof e?e:a,o[1]=s;for(var d=2;d<r;d++)o[d]=n[d];return i.a.createElement.apply(null,o)}return i.a.createElement.apply(null,n)}b.displayName="MDXCreateElement"},43:function(e,t,n){"use strict";n.r(t),n.d(t,"frontMatter",(function(){return r})),n.d(t,"metadata",(function(){return o})),n.d(t,"rightToc",(function(){return s})),n.d(t,"default",(function(){return d}));var a=n(3),i=(n(0),n(107));const r={title:"LidarsAndCamerasSeq"},o={unversionedId:"input-api/inputs/lidars_and_cameras_seq",id:"input-api/inputs/lidars_and_cameras_seq",isDocsHomePage:!1,title:"LidarsAndCamerasSeq",description:"A LidarsAndCamerasSeq input consists of a sequence of camera images and lidar point clouds. Unlike single-frame input types you also have to specify the sequential relationship between the frames, where each frame consists on 1-8 camera images, as well as lidar point clouds.",source:"@site/docs/input-api/inputs/lidars_and_cameras_seq.md",slug:"/input-api/inputs/lidars_and_cameras_seq",permalink:"/annotell-python/docs/input-api/inputs/lidars_and_cameras_seq",editUrl:"https://github.com/annotell/annotell-python/edit/gh-pages/docs-src/docs/input-api/inputs/lidars_and_cameras_seq.md",version:"current",sidebar:"docs",previous:{title:"LidarsSeq \ud83d\udea7",permalink:"/annotell-python/docs/input-api/inputs/lidars_seq"},next:{title:"Calibration",permalink:"/annotell-python/docs/input-api/calibration"}},s=[{value:"Creating a list of frames",id:"creating-a-list-of-frames",children:[]},{value:"Creating the input",id:"creating-the-input",children:[]},{value:"Providing Ego Vehicle Motion Information",id:"providing-ego-vehicle-motion-information",children:[]}],m={rightToc:s};function d({components:e,...t}){return Object(i.mdx)("wrapper",Object(a.default)({},m,t,{components:e,mdxType:"MDXLayout"}),Object(i.mdx)("p",null,"A ",Object(i.mdx)("inlineCode",{parentName:"p"},"LidarsAndCamerasSeq")," input consists of a sequence of camera images and lidar point clouds. Unlike single-frame input types you also have to specify the sequential relationship between the ",Object(i.mdx)("em",{parentName:"p"},"frames"),", where each frame consists on 1-8 camera images, as well as lidar point clouds."),Object(i.mdx)("div",{className:"admonition admonition-info alert alert--info"},Object(i.mdx)("div",{parentName:"div",className:"admonition-heading"},Object(i.mdx)("h5",{parentName:"div"},Object(i.mdx)("span",{parentName:"h5",className:"admonition-icon"},Object(i.mdx)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"},Object(i.mdx)("path",{parentName:"svg",fillRule:"evenodd",d:"M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"}))),"no multi-lidar support currently")),Object(i.mdx)("div",{parentName:"div",className:"admonition-content"},Object(i.mdx)("p",{parentName:"div"},"Currently there is only support for supplying a single point cloud per frame."))),Object(i.mdx)("h2",{id:"creating-a-list-of-frames"},"Creating a list of frames"),Object(i.mdx)("p",null,"The sequential relationship is expressed via a list of ",Object(i.mdx)("inlineCode",{parentName:"p"},"Frame")," objects. This representation expresses the ordering of the frames, but it does not include the ",Object(i.mdx)("em",{parentName:"p"},"relative temporal")," relationship between the frames in the list."),Object(i.mdx)("pre",null,Object(i.mdx)("code",{parentName:"pre",className:"language-python"},"frame_1 = IAM.Frame(...)\nframe_2 = IAM.Frame(...)\nframes = [frame_1, frame_2]\n")),Object(i.mdx)("p",null,"In other words, this representation captures that ",Object(i.mdx)("inlineCode",{parentName:"p"},"frame_1")," comes before ",Object(i.mdx)("inlineCode",{parentName:"p"},"frame_2"),", but does not express how much time has passed between the two frames. In order to express how much time has passed between the frames we need to specify the field ",Object(i.mdx)("inlineCode",{parentName:"p"},"relative_timestamp")," for each frame object. If we for example know that we have collected and aggregated our sensor data at 2Hz, then we would express that as"),Object(i.mdx)("pre",null,Object(i.mdx)("code",{parentName:"pre",className:"language-python"},"frame_1 = IAM.Frame(..., relative_timestamp=0)\nframe_2 = IAM.Frame(..., relative_timestamp=500)\nframes = [frame_1, frame_2]\n")),Object(i.mdx)("div",{className:"admonition admonition-tip alert alert--success"},Object(i.mdx)("div",{parentName:"div",className:"admonition-heading"},Object(i.mdx)("h5",{parentName:"div"},Object(i.mdx)("span",{parentName:"h5",className:"admonition-icon"},Object(i.mdx)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"12",height:"16",viewBox:"0 0 12 16"},Object(i.mdx)("path",{parentName:"svg",fillRule:"evenodd",d:"M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"}))),"Why is relative time important?")),Object(i.mdx)("div",{parentName:"div",className:"admonition-content"},Object(i.mdx)("p",{parentName:"div"},"Specifying the time relationship between frames is important in order to enable different types of advanced annotator assistance tools in the Annotell platform. These tools enable a ",Object(i.mdx)("strong",{parentName:"p"},"significant")," reduction in annotation time, while keeping quality high."))),Object(i.mdx)("h2",{id:"creating-the-input"},"Creating the input"),Object(i.mdx)("p",null,"In addition to supplying the sequential and temporal information for our ",Object(i.mdx)("inlineCode",{parentName:"p"},"Frame")," objects we also need to specify the camera images and lidar point clouds that constitute each frame. This is done by passing a list of ",Object(i.mdx)("inlineCode",{parentName:"p"},"PointCloudFrame")," and ",Object(i.mdx)("inlineCode",{parentName:"p"},"ImageFrame")," objects, where each of these objects contains the path to the underlying file as well as the sensor name. Finally, we also need to specify the ",Object(i.mdx)("inlineCode",{parentName:"p"},"frame_id")," of each frame."),Object(i.mdx)("p",null,"In order to create the input we need to use our list of ",Object(i.mdx)("inlineCode",{parentName:"p"},"Frame")," objects and specify the parameters ",Object(i.mdx)("inlineCode",{parentName:"p"},"external_id")," and ",Object(i.mdx)("inlineCode",{parentName:"p"},"calibration_id")," as well as the optional parameter ",Object(i.mdx)("inlineCode",{parentName:"p"},"sensor_specification"),"."),Object(i.mdx)("table",null,Object(i.mdx)("thead",{parentName:"table"},Object(i.mdx)("tr",{parentName:"thead"},Object(i.mdx)("th",{parentName:"tr",align:null},"Parameter"),Object(i.mdx)("th",{parentName:"tr",align:null},"Description"))),Object(i.mdx)("tbody",{parentName:"table"},Object(i.mdx)("tr",{parentName:"tbody"},Object(i.mdx)("td",{parentName:"tr",align:null},"external_id"),Object(i.mdx)("td",{parentName:"tr",align:null},"Id which can be used to track progress of annotations with.")),Object(i.mdx)("tr",{parentName:"tbody"},Object(i.mdx)("td",{parentName:"tr",align:null},"sensor_specification"),Object(i.mdx)("td",{parentName:"tr",align:null},"Additional information about sensors, includes ",Object(i.mdx)("inlineCode",{parentName:"td"},"sensor_to_pretty_name")," and ",Object(i.mdx)("inlineCode",{parentName:"td"},"sensor_order"),". Defines which sensor that should be shown first, the sensor_order, or a mapping of sensor names to a prettier name version displayed in the Annotell Annotation Tool.")),Object(i.mdx)("tr",{parentName:"tbody"},Object(i.mdx)("td",{parentName:"tr",align:null},"calibration_id"),Object(i.mdx)("td",{parentName:"tr",align:null},"Which calibration to use for the input.")))),Object(i.mdx)("pre",null,Object(i.mdx)("code",{parentName:"pre",className:"language-python",metastring:"reference",reference:!0},"https://github.com/annotell/annotell-python/blob/master/annotell-input-api/examples/lidars_and_cameras_seq.py\n")),Object(i.mdx)("div",{className:"admonition admonition-note alert alert--secondary"},Object(i.mdx)("div",{parentName:"div",className:"admonition-heading"},Object(i.mdx)("h5",{parentName:"div"},Object(i.mdx)("span",{parentName:"h5",className:"admonition-icon"},Object(i.mdx)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"},Object(i.mdx)("path",{parentName:"svg",fillRule:"evenodd",d:"M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"}))),"Be careful with sensor names")),Object(i.mdx)("div",{parentName:"div",className:"admonition-content"},Object(i.mdx)("p",{parentName:"div"},"Make sure that the provided sensor names for each image and lidar frame are present in the calibration supplied to the input. Otherwise the input cannot be created. For more information about this see the ",Object(i.mdx)("a",{parentName:"p",href:"/annotell-python/docs/input-api/calibration"},"Calibration")," section."))),Object(i.mdx)("div",{className:"admonition admonition-tip alert alert--success"},Object(i.mdx)("div",{parentName:"div",className:"admonition-heading"},Object(i.mdx)("h5",{parentName:"div"},Object(i.mdx)("span",{parentName:"h5",className:"admonition-icon"},Object(i.mdx)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"12",height:"16",viewBox:"0 0 12 16"},Object(i.mdx)("path",{parentName:"svg",fillRule:"evenodd",d:"M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"}))),"reuse calibration")),Object(i.mdx)("div",{parentName:"div",className:"admonition-content"},Object(i.mdx)("p",{parentName:"div"},"Note that you can, and should, reuse the same calibration for multiple inputs if possible."))),Object(i.mdx)("div",{className:"admonition admonition-tip alert alert--success"},Object(i.mdx)("div",{parentName:"div",className:"admonition-heading"},Object(i.mdx)("h5",{parentName:"div"},Object(i.mdx)("span",{parentName:"h5",className:"admonition-icon"},Object(i.mdx)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"12",height:"16",viewBox:"0 0 12 16"},Object(i.mdx)("path",{parentName:"svg",fillRule:"evenodd",d:"M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"}))),"Use dryrun to validate input")),Object(i.mdx)("div",{parentName:"div",className:"admonition-content"},Object(i.mdx)("p",{parentName:"div"},"Setting ",Object(i.mdx)("inlineCode",{parentName:"p"},"dryrun")," parameter to true in the method call, will validate the input using the Input API but not create any inputs."))),Object(i.mdx)("h2",{id:"providing-ego-vehicle-motion-information"},"Providing Ego Vehicle Motion Information"),Object(i.mdx)("p",null,"Ego vehicle motion (i.e. the position and rotation of the ego vehicle) is optional information that can be provided when creating ",Object(i.mdx)("inlineCode",{parentName:"p"},"LidarsAndCamerasSeq")," inputs. This information can enable a massive reduction in the time it takes to annotate static objects. Ego vehicle motion information is provided by passing a ",Object(i.mdx)("inlineCode",{parentName:"p"},"EgoVehicleMotion")," object to ",Object(i.mdx)("strong",{parentName:"p"},"each")," ",Object(i.mdx)("inlineCode",{parentName:"p"},"Frame")," in the input."),Object(i.mdx)("pre",null,Object(i.mdx)("code",{parentName:"pre",className:"language-python",metastring:"reference",reference:!0},"https://github.com/annotell/annotell-python/blob/master/annotell-input-api/examples/lidars_and_cameras_seq_full.py\n")),Object(i.mdx)("div",{className:"admonition admonition-note alert alert--secondary"},Object(i.mdx)("div",{parentName:"div",className:"admonition-heading"},Object(i.mdx)("h5",{parentName:"div"},Object(i.mdx)("span",{parentName:"h5",className:"admonition-icon"},Object(i.mdx)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"},Object(i.mdx)("path",{parentName:"svg",fillRule:"evenodd",d:"M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"}))),"Coordinate Systems")),Object(i.mdx)("div",{parentName:"div",className:"admonition-content"},Object(i.mdx)("p",{parentName:"div"},"Note that both ",Object(i.mdx)("inlineCode",{parentName:"p"},"position")," and ",Object(i.mdx)("inlineCode",{parentName:"p"},"rotation")," for ego vehicle pose are with respect to the ",Object(i.mdx)("em",{parentName:"p"},"local")," coordinate system."))))}d.isMDXComponent=!0}}]);